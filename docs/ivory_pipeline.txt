Protocol for analyzing new elephant ivory seizure data

This pipeline is specific for African elephants with the 16
microsatellite data set.  It will need modification to accomodate
other species or types of data.

Code and reference data for this pipeline are in the private GitHub
repository https://github.com/mkkuhner/ivory_pipeline.  Within that
repository, programs and scripts are in the src directory,
additional files needed to run the scripts are in the auxillary_files
directory, and documentation files are in the docs directory.

WARNING:  Never check anything into the GitHub repository which is
law enforcement sensitive or otherwise high security.  

The pipeline also relies on the following code found in other
respositories:

SCAT -- https://github.com/stephens999/scat
VORONOI -- https://github.com/stephens999/voronoi
EBhybrids -- https://github.com/stephenslab/EBhybrids
Familial Matching -- https://github.com/cwolock/elephant_fam_match
Structure -- https://web.stanford.edu/group/pritchardlab/structure_software/release_versions/v2.3.4/html/structure.html

Currently the ivorydata/src directory does not include Structure, SCAT or
VORONOI, but does include copies of EBhybrids and Familial Matching.

All Python scripts are Python3 except for step F3 (programs plot_scat_vor.py
and plot_scat.py), which must be run in Python2 as they use the "basemap" 
library.  We do not currently have a workaround if "basemap" is unavailable,
and as it is deprecated, it may not be available for much longer.  

***General Rules

1)  The term [species] refers to either savannah or forest.

2)  The term [date] refers to a date in format YYYY_MM_DD (e.g.
2021_01_30.

3)  The term [DBno] refers to the numeric ID of a release of the Master Elephant Genotypes
file.

***A)  Obtaining all needed files

1)  If you do not have up-to-date installations of the following programs:

SCAT2
VORONOI
EBhybrids
Familial Matching
Structure

obtain and install them now.  Detailed instructions are in document 
installing_software.txt in the ivory_pipeline directory.

2)  Obtain the needed data files.  Files with elephant genotypes, reference or 
seizure, are not kept in the online repositories for security reasons.  Yves 
Horeau can provide the most recent copy of the master elephant genotypes file, 
which can be used as a source for reference data and previously analyzed 
seizure data.  Data for a brand new seizure is generally not yet in this file; 
it should be requested from the laboratory staff who genotyped it.

You will minimally need the Master Elephant Genotype file, which is 
distributed as a .xlsx file, unfortunately with variations in its name.  
It may be something like:

Elephant_Genotypes_Master_[DBno].xlsx
ElephantMasterFile[DBno].xlsx

If your seizure has not yet been added to this file, you will also need a 
data file containing its genotypes; this will generally be an .xlsx file 
but its name is arbitrary.  (Also ask them what the official name of the 
new seizure is.)

***B)  Obtaining the reference data.

If you already have SCAT-style formatted reference data and the 
corresponding Structure results, you can skip this step.

These files would be called:

REFELE_[DBno]_raw.txt                  (raw reference data)
REFELE_[DBno]_known.txt                (formatted reference data)
REFELE_[DBno]_known_structure.txt_f    (Structure results)

If you do not have these files, steps to prepare them are in 
/docs/reference_pipeline.txt.

***C)  Preprocessing the data for analysis

1)  Pick a PREFIX (ideally the official name of the seizure) that will 
tag all runs for this case.

2)  Obtain the seizure genotype data.  For a new seizure, open the
spreadsheet and look for a tab labeled "Scat input".  To use data
from older seizures which are already in the database, refer to the 
documentation file master_database_pipeline.txt, which will replace step A3.

3)  Extract the "Scat input" data from the spreadsheet as a .tsv file; this
can be done with LibreOffice by saving the file as .csv but setting the 
delimiter to tab rather than comma, and then changing the name of the
resulting file.  Name this file PREFIX_raw.tsv.

**OPTIONAL**

Steps C4 through E4 can be run with a shell script.  On Mary's laptop
use mary_setup_seizure.sh; on Jon's use jon_setup_seizure.sh.  The arguments
are PREFIX and a path to the location of PREFIX_raw.tsv.  If you run this
script you will then be ready to do the SCAT2 runs (step E5) in
nforest, nsavannah, or both, depending on which types of elephants were found
in your seizure.

**

4)  Validate the microsat data by running verifymsats.py with the following
arguments:
--number of msats (currently 16)
--REFELE_[DBno]_raw.csv
--PREFIX_raw.tsv

This program checks for samples with more than 2 previously unknown alleles.
This can indicate that the microsats have gotten out of order; if it fires,
carefully check your input seizure data.  NOTE:  it only checks the seizure data,
and assumes the reference data are correct.

5)  Run prep_scat_data.py with an argument of PREFIX. This will read
PREFIX_raw.tsv and write PREFIX_unknowns.txt.  This program marks the new data 
as region -1, and corrects msats where only one allele is missing to 
mark both as missing.  It also suppresses individuals with less
than 10 successfully typed microsatellites.  (These have often been
suppressed upstream by the author of the spreadsheet, but we double-
check here.)


***D)  Assigning species and identifying hybrids

This step relies on Structure results for the reference data; these
will be called REFELE_[DBno]_known_structure.txt_f.  The Structure
results MUST be for exactly the version of the database that you are
using; even a single elephant change will invalidate them.  If in
doubt, rerun.

1)  Obtain the appropriate allelic dropout file.  This is
currently "dropoutrates_savannahfirst.txt" and is in the auxillary_files 
directory.  (It is a columns reversed version of the original file from 
Mondol et al, which was named "Mondoletal_allelicdropoutrates.txt".)

Note:  In theory, you can create this file for novel data using R scripts, 
but I have never gotten them to run successfully.  The R scripts you would 
use are in the /src directory:

infermarkerspecnullalleleprob.R
infernullalleleprobanderrorprob.R

2)  Run make_eb_input.py.  Argument 1 is the Structure output file.  Argument 2
is REFELE_[DBno]_known.txt, the region-known reference data file.
Argument 3 is PREFIX.  Argument 4 is dropoutrates_savannahfirst.txt.
The script also assumes that file ebscript_template.R is present in its 
directory:  this file can be found in the auxillary_files directory of
the ivory_pipeline repository.  

There are three outputs.  

PREFIX_ancestryprops.txt gives the ancestry proportions needed by EBhybrids.  
Be careful with this file; it does not contain sample IDs, but relies on 
being in the same order as the other files written by make_eb_input.py.

PREFIX_plus_ref.txt contains genotype data for the seizure and all reference
individuals (both species).

ebscript.R is the EBhybrids run script modified to point at these files.
Note that if this file exists, it will be overwritten.

3)  Run ebscript.R (command is "Rscript ebscript.R").  This will run
EBhybrids and write four files:  PREFIX_hybt.csv and .txt, and PREFIX_HPs.csv
and .txt.  The hybt files give the probability that the sample is either pure
species or any of 4 hybrid types.  The HPs files give only the probability that
it is a hybrid.  Information is identical between .csv and .txt.


***E) Running SCAT2

We do not recommend using the built-in species ranges in SCAT2 or
the old-style boundary file 316forestboundary.txt.   The programs will
run, but the resulting boundaries are very poor (they include ocean and
exclude valid habitat).

1)  Obtain the following files and place in current directory:

-- Map files of choice; either the "full" map files mapfile_161220_[species].txt
or the IUCN map files iucn_161220_[species].txt.  These are found in the
auxillary_files directory.

-- Region file of choice.  The current version is regionfile.v38b.txt. 
This is found in the auxillary_files directory.  The regionfile gives the
mapping between sampling location ("region") name, number, subregion,
and latitude/longitude.  If new sampling regions have been added to the
reference data, you will need to update this file and create a new
version.

-- Prototype SCAT2 and VORONOI run files master_scat_runfile.sh and
master_voronoi_runfile.sh.  These are found in the auxillary_files
directory; they are templates for creating run commands for your
SCAT2 and VORONOI runs.

OPTIONAL:  If you want a different SCAT2 run length than our standard one, 
edit the master_scat_runfile.sh file.  The last three numbers (currently 
100 20 100) control the length of the run:  the first is number of sampled 
iterations, the second is steps between samplings, and the third is burnin.
VORONOI assumes exactly 100 sampled iterations and 100 non-sampled 
iterations and would have to be modified to change this, so your best bet is
to change the steps between samplings.  Runtime will increase linearly with 
this (if you double steps between samplings, runtime will approximately 
double).

2)  Run filter_hybrids.py with the following arguments:

-- PREFIX of the run
-- Map prefix, for example mapfile_161220 or iucn_161220 (the part of
the mapfile name before the species name).
-- Regionfile name.  
-- Reference data prefix REFELE_[DBno]
-- T if the SCAT runs will be done on the cluster, F otherwise

The filter_hybrids.py program will print its hardwired hybrid cutoff to
screen.  Make sure this cutoff is what you want.

This program uses PREFIX_hybt.txt to sort the samples into, maximally, a 
forest and a savannah SCAT input file, called PREFIX_[species].txt;
if there are no unknown individuals from a species, that file will not be 
written.  It discards individuals called as hybrids.  It also writes
a file with all non-hybrid seizure and ref individuals, called
PREFIX_conjoint_nohybrids.txt, for use in familial matching.

Outputs:

SCAT run files:  runfile_[species].sh 
VORONOI run files:  voronoi_runfile_[species].sh
SCAT-format data files:  PREFIX_[species].txt for each species present,
  and PREFIX_conjoint_nohybrids.txt for familial matching

3)  Create a directory for each species-specific SCAT2 analysis.  All 
subsequent steps will have to be done for both directories, if you have both
species in your seizure.  (We typically name these directories "nforest"
and "nsavannah" but this is not required.)  Copy the corresponding SCAT 
input files (PREFIX_[species].txt) into these directories.  Also copy 
in the corresponding Voronoi runfiles (voronoi_runfile_[species].sh).
However, leave the SCAT run files where they are.

Place a SCAT2 executable in this species-specific SCAT2 directory.  We 
recommend a clean optimized build from source.  

4)  Run setupscatruns.py with the following arguments (twice, if you
have both forest and savannah individuals):

-- name of the species-specific run directory
-- name of the master scat run script (runfile_[species].sh) 
-- random number seed (positive integer)

This seed will be used for the first directory and incremented by 1 for each
subsequent directory.

This will create 9 subdirectories named 1-9 under your named directory, each 
of which contains a SCAT2 run command file (called runX.sh where X is the 
directory number) and an /outputs directory to hold the results.

5)  Open a window for each numbered directory and run the runX.sh file (using
"source" as they are not executable files).  This should run SCAT2 nine
times.  It likely will take hours or days to run.


***F)  Running VORONOI

Do not do this step unless there were at least 2 samples in the species
under consideration:  running VORONOI on one sample is not useful.

** OPTIONAL **

The script mary_finish_seizure.sh will run steps F1 through G3.  Its arguments
are PREFIX, the path to a VORONOI executable, and "forest" or "savannah"; it
should be run in /nforest or /nsavannah (or both if both exist).  

The equivalent script jon_finish_seizure.sh only does steps F1 through G1,
as G2 does not run on Jon's laptop due to reliance on Basemap.

**

1)  Obtain or prepare a masterfile consisting of 9 lines as follows (there
is an example, called "masterfile", in the auxillary_files directory) and
place it in the species-specific directory.

1/outputs
2/outputs 
...

2)  In the species-specific directory, run scat2voronoi.py
with the argument being the masterfile from step 1.  This writes multiple
files:

a)  A large number of files with names of the form nnnc where nnn are
digits and c is a character between r and z.  These are single elephant
SCAT result files:  001r is the result for the first elephant in directory
1.  If something goes wrong in subsequent steps, the first thing to check
is whether all of these files are present (often one letter of the
alphabet is missing, which likely indicates that one of the SCAT2 runs
has failed).

b)  Files named samplemap.r through samplemap.z which relate the nnn numbers
to elephant IDs.  In practice we only use samplemap.r; they should all be
identical.

c)  A file voronoiin.txt which instructs VORONOI on which samples (by
nnn identifier) it should run.

3)  Pull a VORONOI executable:  I recommend a clean optimized build from
source.

4)  Run VORONOI by source-ing the run command file present in the
species-specific directory (this file was written by make_species_files.py). 
It should take no more than 10-15 minutes for most data.


***G)  Post-processing


1)  Run prep_reports.py in the directory where you ran VORONOI.  Its
first argument is PREFIX and the second is the voronoiin.txt file you
used for the run.  It also uses the VORONOI output files PREFIX_indprobs and
PREFIX_mapinfo, and the samplemap.r file; it expects these files to be
present in the directory where it is being run (they are not arguments).  
It writes individual VORONOI grid files to a directory named PREFIX_reports.

2)  Run plot_scat_vor.py in the directory where you ran VORONOI.  It
takes 1 or 2 arguments.  If there is only 1 it is PREFIX, and all samples
will be run.  If there are 2, they are PREFIX and SID, and only the
sample with that SID will be run.  This fills up PREFIX_reports with
graphics of the SCAT2 and VORONOI surfaces for each sample, plus three
summary files:  

scat_summary_medians.jpg (SCAT2 results as median latitude/longitude)
scat_summary_squares.jpg (SCAT2 results as grid square of highest count)
voronoi_sumary.jpg (VORONOI results as grid square of highest posterior)

If you need SCAT2 graphics without VORONOI graphics, for example because
there was only 1 sample for this species, they can be gotten with
plot_scat.py with the same arguments.

NOTE:  Neither plot program for step 3 will run in Python3 as they
use Basemap.  They will not run on Jon's laptop at all.  They are
likely to stop working at all in the near future as Basemap is
being retired.

TO DO:  Convert to Cartopy library.

3)  OPTIONAL but recommended:  In the directory where you ran VORONOI,
run cleanup_voronoi.py with no arguments.  This deletes duplicate copies
of the SCAT2 output files, placed in this directory for use by VORONOI,
and saves a lot of space.  (These files are named 001r, 001s, etc.)  If 
you find that you need those files back, they can be recreated by rerunning 
scat2voronoi.py.


***H)  Run familial matching

This document explains how to do familial matching of a new seizure
all previous seizures have already been run and archived.  If
you need to do familial matching on the entire data set, see documentation
file familial_pipeline.txt, and be aware that it will take 3+ days.
Rerunning the whole thing is needed if large changes have been made to
the reference data, the subregion definitions have changed, or the set
of previous seizures to be used has changed.

1)  Obtain the file seizure_metadata_[DBno].tsv (this is not kept
on the Github site; ask Mary Kuhner for a copy).  Make a COPY of this
file and run update_metadata.py on the copy (this program changes its
input file in place!)  The three arguments are the metadata (copy) file,
the PREFIX_unknowns.txt file, and the official name of your new seizure.
Check that the results are sensible (your new seizure should be at
the bottom of the file) before overwriting the original.  If you have 
changed [DBno] be sure to change the name of this file.  Return the
modified file to Mary for archiving.

2)  In your main PREFIX directory (not nforest/nsavannah), create a SCAT 
output directory "outdir".  Run SCAT2 using the following
command line:

./SCAT2 -Z -H2 PREFIX_conjoint_nohybrids.txt regionfile outdir 16

This will compute subregion probabilities for all reference and seizure
samples.  The seizure samples will be classified into subregions based
on these probabilities:  the reference samples will instead be classified
based on their subregion in the regionfile, but must be included here
to establish the subregional allele frequencies.  It runs in minutes.

3)  Obtain make_fammatch_incremental.py from the ivory_pipeline /src
directory.  Check to make sure it has the correct value for the
nsub variable (number of subregions, currently 6).  Run this
program with three arguments:

outdir/Output_hybrid file created in step 2
PREFIX_conjoint_nohybrids.txt
regionfile

This will write two files per subregion:  refsN_fammatch.csv and 
newN.txt, where N is the subregion number.  No files will be written
for a subregion which has no samples from the new seizure.

4)  Obtain the "oldN.txt" files covering all previous seizures.  These
are not archived on Github for security reasons:  ask Mary for a copy.

5)  Place copies of calculate_LRs.R, LR_functions.R, and 
setupivoryfammatch.sh, found in the /src directory, in the current 
directory.  Run setupivoryfammatch.sh.  This creates directories named 
"subN" for each subregion N for which seizure samples were found, and 
moves all needed files to that directory.  It also writes a script 
"runrscript.sh" into each directory.

This script assumes that subregions 0 and 1 are forest and all others 
are savannah, and will need to be revised if that changes.

6)  In each subregion directory, source runrscript.sh (no arguments).
This will contain a run command of the form:

Rscript calculate_LRs.R [species] refN_fammatch.csv oldN.txt newN.txt

Runtime depends on the size of the seizure but usually not more than
a few hours.  You can run all of them simultaneously.


***I)  Postprocess familial matching

1) OPTIONAL.  To simply view the results of familial matching on your new 
seizure, you can run Charles' Python scripts in each subregional directory for
which you had samples in your seizure.  These are found in the /src directory.
If no matches are found, there is no point continuing with network analysis.
Be careful, however, not to treat matches found this way as conclusive; a
large proportion will be false positives.

python 1_add_seizures.py --input_file obsLRs.[species].txt --seizure_file seizure_metadata.tsv

This writes obsLRs.[species].seizures.txt which is annotated with seizure
names.  As it does not use seizure_modifications, it will not necessarily get the
names right.

python 2_filter_results.py --input_file obsLRs.[species].seizures.txt --cutoff 2.0

Note that if you mistakenly use obsLRs.[species].txt as input, this program will
appear to work fine, but downstream code will NOT.   It writes
obsLRs.[species].2.0.filtered.txt, which can be consulted to find all the putative
significant matches involving your new seizure.

python 3_seizure_analysis.py --filtered_file obsLRs.[species].seizures.2.0.filtered.txt

This will write obsLRs.[species].seizures.2.0.filtered.seizanalysis.txt, which
summarizes comparisons between seizures.  For interpreting the results you
will use both obsLRs.[species].2.0.filtered.txt and this file.

(There is a fourth script distributed with the familial matching code,
4_remove_seizures.py, but this pipeline does not use it.  It is also
present in src if you find a use for it.)

1)  Obtain or create a seizure_modifications file.  This file contains
information on any seizures to be excluded from analysis or merged
together.  A copy of the current version, seizure_modifications_nature,
is in the auxillary_files directory.  It reflects the seizure changes
made for the paper submitted to Nature in May 2021.

This file has two sections.  The first is introduced with REJECT
on a line by itself and lists seizures to be rejected, one per line.
The second is introduced with MERGE on a line by itself and
introduces tab-separated lists of seizure names.  The first name will
be used as the name of the merged seizure, and the remainder will be
merged.  Note that you can use this to rename a seizure by giving
two entries, the first being the new name and the second the old
name.

It is likely that if you change this file at all, except as it pertains
to the new seizure you are running, you will have to rerun the
entire familial matching analysis as described in familial_pipeline.txt.
You *may* be able to reject more seizures than were rejected when your
previous familial matching runs were done, though this is not tested;
similarly, you may be able to rename old seizures.  You certainly 
cannot reject fewer old seizures or merge old seizures with each other 
or with the new ones.


3)  Obtain results of previous familial matching.  They will take the form of 
files named obsLRs_[species]_N.txt, where N is the subregion number.  These are
archived by Mary.  Place them in the directory which contains your subregional
directories sub0, sub1 etc.

Obtain a file of false-positive results from simulated data, called fprates.tsv;
a copy is in auxillary_files.  If the subregioning scheme is changed, these will
need to be regenerated.

Obtain a copy of the canonical list of direct (exact) matches, called dms.tsv.
This is not kept on Github; ask Mary for a copy.  This file is used because we 
recognize as direct matches some imperfect matches which have been detected by
allelic-dropout-aware code such as CERVUS, whereas allowing for imperfect
matches in the familial matching code would make it too slow.

4)  Run PROGRAM TO BE WRITTEN to merge old and new results.

5)  Run create_network_input.py to calculate weighted matches between seizures.
Its arguments are:

seizure_metadata.txt
dmfile.tsv
seizure_modifications
log(LR) cutoff (standard is 2.0)
minimum typed loci in common (standard is 13)

The program also silently reads fprates.tsv, which must be in its run directory;
and it contains hardcoded path names for the obsLRs files (see line 63 and following)
which will have to be changed if you use a different directory setup.

It writes seizure_nodes.csv and seizure_edges.csv, which contain the weighted
matches between each pair of seizures.  It also writes two matrices of counts
between pairs of seizures, broken out by forest and savannah, into a single
file named "ryanfile" used by Ryan's graphical software.

If you simply want to know about all weighted matches involving the new seizure,
grep for its name in seizure_edges.csv.

6)  To infer a Louvain network and make a graphic of it, run the Python program
infer_louvain_network.py.  Arguments are:

minlink -- connections with weight below this are dropped.  Was 1.0 in paper.
seizure_modifications

There are fancier versions of this program which pull in forensic data, Ryan's
data, etc. but this is the basic version.
