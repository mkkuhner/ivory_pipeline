Protocol for analyzing new elephant ivory seizure data

This pipeline is specific for African elephants with the 16
microsatellite data set.  It will need modification to accomodate
other species or types of data.

Code and reference data for this pipeline are in the private GitHub
repository https://github.com/mkkuhner/ivory_pipeline.  Within that
repository, programs and scripts are in the src directory,
additional files needed to run the scripts are in the auxillary_files
directory, reference data are in the data directory, and documentation 
files are in the docs directory.

WARNING:  Never check anything into the GitHub repository which is
law enforcement sensitive or otherwise high security.  

The pipeline also relies on the following code found in other
respositories:

SCAT -- https://github.com/stephens999/scat
VORONOI -- https://github.com/stephens999/voronoi
EBhybrids -- https://github.com/stephenslab/EBhybrids
Familial Matching -- https://github.com/cwolock/elephant_fam_match
Structure -- https://web.stanford.edu/group/pritchardlab/structure_software/release_versions/v2.3.4/html/structure.html

Currently the ivorydata/src directory does not include Structure, SCAT or
VORONOI, but does include copies of EBhybrids and Familial Matching.

All Python scripts are Python3 except for step F3 (programs plot_scat_vor.py
and plot_scat.py), which must be run in Python2 as they use the "basemap" 
library.  We do not currently have a workaround if "basemap" is unavailable,
and as it is deprecated, it may not be available for much longer.  

***General Rules

1)  The term [species] refers to either savannah or forest.

2)  The term [date] refers to a date in format YYYY_MM_DD (e.g.
2021_01_30.

3)  The term [year] refers to a two-digit year (e.g. 21).


***A)  Preprocessing the data for analysis

1)  Pick a PREFIX (ideally the official name of the seizure) that will 
tag all runs for this case.

2)  Obtain the seizure genotype data.  For a new seizure, open the
spreadsheet and look for a tab labeled "Scat input".  To use data
from older seizures, refer to the documentation file 
master_database_pipeline.txt, which will replace step A3.

3)  Extract the "Scat input" data from the spreadsheet as a .tsv file; this
can be done with LibreOffice by saving the file as .csv but setting the 
delimiter to tab rather than comma, and then changing the name of the
resulting file.  Name this file PREFIX_raw.tsv.

4)  Run prep_scat_data.py with an argument of PREFIX. This will read
PREFIX_raw.tsv and write PREFIX_unknowns.txt.  This program marks the new data 
as region -1, and corrects msats where only one allele is missing to 
mark both as missing.  It also suppresses individuals with less
than 10 successfully typed microsatellites.  (These have often been
suppressed upstream by the author of the spreadsheet, but we double-
check here.)


***B)  Preparing a Structure file for the reference data set.

This step should only be done if the reference data have changed (for example
adding or removing reference individuals or regions).  Otherwise, use the
previously run results.

1) Make a reference file for Structure by prepending header_for_structure
(in the auxillary_files directory) to REFELE_[year]_known.txt and naming
it REFELE_[year]_structure.txt.

2)  Edit the "mainparams" file (in the auxillary_files directory)
to set the INFILE to REFELE_[year]_structure.txt and the OUTFILE to 
REFELE_[year]_known_structure.txt.  Set NUMINDS to the total individuals in your
reference set (bearing in mind that the input file has 2 lines per
individual).  Set NUMLOCI to your number of loci (16 for standard elephant
microsat data) and MISSING to "-999".

3)  Run Structure.  You do not need to set any command line options as they
are handled by "mainparams".  Your output file will have the name you gave
it in "mainparams" followed by "_f".  This takes minutes.

4)  Archive the resulting file for future use.

WARNING:  Structure makes an arbitrary choice whether forest or savannah is
Species 1.  This is corrected by the downstream script make_eb_input.py
so that savannah is always Species 1, but the Structure results themselves
are not modified.  If you use this file for anything other than the pipeline,
you must diagnose which species is Species 1.  Our canonical way to do this 
is to check elephant CH0878, a clearly savannah reference elephant; whichever 
column (1 or 2) of Structure results has high values for this individual is 
savannah.  


***C)  Assigning species and identifying hybrids

This step assumes that Structure results for the reference data are available.
(We do not run Structure on the seizure data.)  There should be an archived 
version of the Structure results for reference in the data directory, 
named REFELE_[year]_known_structure.txt_f; if this file does not exist go 
to step B and make it.  IMPORTANT:  The reference data used below MUST
be exactly the same as the reference data on which Structure was run!
If in doubt, rerun Structure.

1)  Obtain the appropriate allelic dropout file.  This is
currently "dropoutrates_savannahfirst.txt" and is in the data directory.  
(It is a columns reversed version of the original file from Mondol et al, 
which was named "Mondoletal_allelicdropoutrates.txt".)

Note:  In theory, you can create this file for novel data using R scripts, 
but I have never gotten them to run successfully.  The R scripts you would 
use are:

infermarkerspecnullalleleprob.R
infernullalleleprobanderrorprob.R

2)  Run make_eb_input.py.  Argument 1 is the Structure output file.  Argument 2
is REFELE_[year]_known.txt, the region-known reference data file.
Argument 3 is PREFIX.  Argument 4 is dropoutrates_savannahfirst.txt.
The script also assumes that file ebscript_template.R is present in its 
directory.  

There are three outputs.  

PREFIX_ancestryprops.txt gives the ancestry proportions needed by EBhybrids.  
Be careful with this file; it does not contain sample IDs, but relies on 
being in the same order as the other files written by make_eb_input.py.

PREFIX_plus_ref.txt contains genotype data for the seizure and all reference
individuals (both species).

ebscript.R is the EBhybrids run script modified to point at these files.
Note that if this file exists, it will be overwritten.

3)  Run ebscript.R (command is "Rscript ebscript.R").  This will run
EBhybrids and write four files:  PREFIX_hybt.csv and .txt, and PREFIX_HPs.csv
and .txt.  The hybt files give the probability that the sample is either pure
species or any of 4 hybrid types.  The HPs files give only the probability that
it is a hybrid.  Information is identical between .csv and .txt.


***D) Running SCAT2

We do not recommend using the built-in species ranges in SCAT2 or
the old-style boundary file 316forestboundary.txt.   The programs will
run, but the resulting boundaries are very poor (they include ocean and
exclude valid habitat).

1)  Obtain the following files and place in current directory:

-- Map files of choice; either the "full" map files mapfile_161220_[species].txt
or the IUCN map files iucn_161220_[species].txt.  These are found in the
data directory.

-- Region file of choice.  This should be regionfile.v36.txt if you used the
most recent version of the reference data (REFELE_20_known.txt) in step C 
(recommended) and regionfile.v35.txt if you used a previous version.
This is found in the data directory.

-- Prototype SCAT2 run file master_scat_runfile.sh.  This will be used as a basis
for runfiles for your SCAT2 runs.   This is found in the auxillary_files
directory.

-- Prototype VORONOI run master file master_voronoi_runfile.sh.
This is found in the auxillary_files directory.

OPTIONAL:  If you want a different run length than our standard one, edit the
master_scat_runfile.sh file.  The last three numbers (currently 100 20 100)
control the length of the run:  the first is number of sampled iterations, the 
second is steps between samplings, and the third is burning.  Downstream
code assumes exactly 100 sampled iterations and 100 non-sampled iterations and
would have to be modified to change this, so your best bet may be changing the
steps between samplings.  Runtime will increase linearly with this (if you
double steps between samplings, runtime will approximately double).

2)  Run make_species_files.py with the following arguments:

-- PREFIX of the run
-- Map prefix, for example mapfile_161220 or iucn_161220 (the part of
the mapfile name before the species name).
-- Regionfile name.  
-- Reference data prefix, for example REFELE_21.

The make_species_files.py program will print its hardwired hybrid cutoff to
screen.  Make sure this cutoff is what you want.

This program uses PREFIX_hybt.txt to sort the samples into, maximally, a forest 
and a savannah SCAT input file, called PREFIX_[species].txt;
if there are no unknown individuals from a species, that file will not be written.
It discards individuals called as hybrids.  It writes one or two
SCAT run files:  runfile_[species].sh.  It also writes one or two VORONOI 
runfiles:  voronoi_runfile_[species].sh.  Finally, it writes one or two 
species-specific versions of the reference file for use by the familial matching 
code (REFELE_[year]_[species]_long.csv) and a corresonding version of the 
samples (PREFIX_genotypes_[species]_wide.csv).

3)  Create a directory for each species-specific SCAT2 analysis.  All subsequent
steps will have to be done for both directories, if you have both
species in your seizure.  Copy the corresponding SCAT input files 
(PREFIX_[species].txt) into these directories.  Also copy in the corresponding 
Voronoi runfiles (voronoi_runfile_[species].sh).

Place a SCAT2 executable in this species-specific SCAT2 directory.  I recommend a 
clean optimized build from source.  

4)  Run setupscatruns.py with the following arguments:

-- name of the run directory set up in 2
-- name of the master scat run script (runfile_[species].sh) 
-- random number seed (positive integer)

This seed will be used for the first directory and incremented by 1 for each
subsequent directory.

This will create 9 subdirectories named 1-9 under your named directory, each 
of which contains a SCAT2 run command file (called runX.sh where X is the 
directory number) and an /outputs directory to hold the results.

5)  Open a window for each numbered directory and run the runX.sh file (using
"source" as they are not executable files).  This should run SCAT2 nine
times.  It will take hours or days to run.


***E)  Running VORONOI

Do not do this step unless there were at least 2 samples in the species
under consideration:  running VORONOI on one sample is not useful.

1)  Obtain or prepare a masterfile consisting of 9 lines as follows (there
is an example, called "masterfile", in the auxillary_files directory) and
place it in the species-specific directory.

1/outputs
2/outputs 
...

2)  In the species-specific directory, run scat2voronoi.py
with the argument being the masterfile from step 1.  This writes multiple
files:

a)  A large number of files with names of the form nnnc where nnn are
digits and c is a character between r and z.  These are single elephant
SCAT result files:  001r is the result for the first elephant in directory
1.

b)  Files named samplemap.r through samplemap.z which relate the nnn numbers
to elephant IDs.  In practice we only use samplemap.r; they should all be
identical.

c)  A file voronoiin.txt which instructs VORONOI on which samples (by
nnn identifier) it should run.

3)  Pull a VORONOI executable:  I recommend a clean optimized build from
source.

4)  Run VORONOI by source-ing the run command file present in the
species-specific directory (this file was written by make_species_files.py). 
It should take no more than 10-15 minutes for most data.


***F)  Post-processing


1)  Run prep_reports.py in the directory where you ran VORONOI.  Its
first argument is PREFIX and the second is the voronoiin.txt file you
used for the run.  It also uses the VORONOI output files PREFIX_indprobs and
PREFIX_mapinfo, and the samplemap.r file; it expects these files to be
present in the directory where it is being run (they are not arguments).  
It writes individual VORONOI grid files to a directory named PREFIX_reports.

2)  Run plot_scat_vor.py in the directory where you ran VORONOI.  It
takes 1 or 2 arguments.  If there is only 1 it is PREFIX, and all samples
will be run.  If there are 2, they are PREFIX and SID, and only the
sample with that SID will be run.  This fills up PREFIX_reports with
graphics of the SCAT2 and VORONOI surfaces for each sample, plus three
summary files:  VORONOI summarized as cell of highest posterior probability,
SCAT2 summarized as median lat/long, and SCAT2 summarized as cell of
highest number of SCAT results.

If you need SCAT2 graphics without VORONOI graphics, for example because
there was only 1 sample for this species, they can be gotten with
plot_scat.py with the same arguments.

NOTE:  Neither plot program for step 3 will run in Python3 as they
use Basemap.  They will not run on Jon's laptop at all.  They are
likely to stop working at all in the near future as Basemap is
being retired.

TO DO:  Convert to Cartopy library.

3)  OPTIONAL but recommended:  In the directory where you ran VORONOI,
run cleanup_voronoi.py with no arguments.  This deletes duplicate copies
of the SCAT2 output files, placed in this directory for use by VORONOI,
and saves a lot of space.  (These files are named 001r, 001s, etc.)  If 
you find that you need those files back, they can be recreated by rerunning 
scat2voronoi.py.


***G)  Familial matching

Recent versions of the familial matching code work with, in addition to
reference data, two files of seizure data:  one containing all previous
seizures, and one containing the new seizure.  Only matches within the
new seizure or between new and old are reported; matches between old
seizures are not.

If you need to get matches between all seizures old and new, this can be done 
by placing just one sample (it does not matter which) in the "old" seizure file
and all other samples in the new seizure file.   Note that this will take
a long time to run (around five days on Mary's laptop).

1)  Make a directory named "fammatch" under each species-specific 
run directory, and copy into it:

The appropriate reference file REFELE_[year]_[species]_long.csv
made in step D2.

The R scripts calculate_LRs.R and LR_functions.R, found in the src
directory.

The python scripts 1_add_seizures.py, 2_filter_results.py, and
3_seizure_analysis.py, found in the src directory.
(There is a fourth script distributed with the familial matching code,
4_remove_seizures.py, but this pipeline does not use it.  It is also
present in src if you find a use for it.)

The python script partition_genotypes.py found in the src directory.

2)  Provide the official name of your seizure and a copy of the file(s)
PREFIX_genotypes_[species]_wide.csv made in step D2 to the
person in charge of updating the seizure genotypes and metadata files.
Receive updated copies of seizure_[species]_wide.csv and 
seizure_metadata.tsv from the maintainer and place them in the appropriate
fammatch directory for their species.

3)  Create input files for familial matching.  Run the Python program
partition_genotypes.py as follows:

python3 partition_genotypes.py [species] PREFIX_genotypes_[species]_wide.csv seizure_[species]_wide.csv

This will write a file for the "old" seizure data, excluding the current
seizure, called other_genoptypes_[species]_wide.csv. 

This program will fail with an error message to screen if your seizure
has not been added to seizure_genotypes.csv and seizure_metadata.tsv.
If this happens, ask the maintainer to correctly update these files, obtain
updated copies, and try again.

4)  Run the familial matching program.  This step will overwrite 
any previous results from familial matching for the same species that are 
present in its directory.  If there are results you wish to keep,
move them away or rename them before running it.

Run calculate_LRs.R (Rscript command).  The arguments are 
[species], REFELE_[year]_[species]_long.csv,
other_genotypes_[species]_wide.csv, and PREFIX_genotypes_[species]_wide.csv.

This writes obsLRs_[species].txt, which contains all pairwise match
scores between old x new seizures and new x new seizures. 

4)  Run 1_add_seizures.py as follows:

python 1_add_seizures.py --input_file obsLRs.[species].txt --seizure_file seizure_metadata.tsv

This writes obsLRs.[species].seizures.txt which is annotated with seizure
names.

5)  Run 2_filter_results.py.  This program filters out non-significant matches
according to a given cutoff.  Internal documentation says that the cutoff is
"on log10 scale" and that it also enforces 10+ loci.  Published work uses
a cutoff of 2.0.  Command line as follows:

python 2_filter_results.py --input_file obsLRs.[species].seizures.txt --cutoff 2.0

Note that if you mistakenly use obsLRs.[species].txt this program will
appear to work fine, but downstream code will NOT.   It writes
obsLRs.[species].2.0.filtered.txt.

6)  Run 3_seizure_analysis.py:

python 3_seizure_analysis.py --filtered_file obsLRs.[species].seizures.2.0.filtered.txt

This will write obsLRs.[species].seizures.2.0.filtered.seizanalysis.txt, which
summarizes comparisons between seizures.  For interpreting the results you
will use both obsLRs.[species].2.0.filtered.txt and this file.
