Pipeline for analyzing new elephant ivory seizure data

This pipeline is specific for African elephants with the 16
microsatellite data set.  It will need modification to accomodate
other species or types of data.  (In particular, the step using
EBhybrids requires exactly two species:  a substitute will be
needed for pangolins if they prove to have hybrids.)

Code and reference data for this pipeline are in the private GitHub
repository https://github.com/mkkuhner/ivory_pipeline.  Within that
repository, programs and scripts are in the /src directory,
additional files needed to run the scripts are in the /aux
directory, and documentation files are in the /docs directory.

WARNING:  Never check anything into the GitHub repository which is
law enforcement sensitive or otherwise high security.  

The pipeline also relies on the following code found in other
respositories:

SCAT -- https://github.com/stephens999/scat
VORONOI -- https://github.com/stephens999/voronoi
EBhybrids -- https://github.com/stephenslab/EBhybrids
Familial Matching -- https://github.com/cwolock/elephant_fam_match
Structure -- https://web.stanford.edu/group/pritchardlab/structure_software/release_versions/v2.3.4/html/structure.html

Currently the ivorydata/src directory does not include Structure, SCAT or
VORONOI, but does include copies of EBhybrids and Familial Matching.
If these programs are updated, replace these copies!

***General Rules

1)  All Python scripts are Python3.

2)  The term [species] refers to either "savannah" or "forest".

3)  The term [date] refers to a date in format YYYY_MM_DD (e.g.  2021_01_30).

4)  The term [DBno] refers to the numeric ID of a release of the Master Elephant 
Genotypes file.

***Automated Scripts

This document covers running the pipeline step by step, by hand.  To run 
it using automated scripts, see document ivory_scripts.txt.


***A)  Obtaining all needed files

1)  If you do not have up-to-date installations of the following programs:

SCAT
VORONOI
EBhybrids
Familial Matching
Structure

obtain and install them now.  Detailed instructions are in document 
installing_software.txt in the ivory_pipeline directory.  I have assumed
that the SCAT and VORONOI executables are named SCAT and VORONOI;
adjust appropriately if they are not. 

2)  Obtain the needed data files.  Files with elephant genotypes, reference or 
seizure, are not kept in the online repositories for security reasons.  Yves 
Horeau can provide the most recent copy of the master elephant genotypes file, 
which can be used as a source for reference data and previously analyzed 
seizure data.  Data for a brand new seizure is generally not yet in this file; 
it should be requested from the laboratory staff who genotyped it.

You will minimally need the Master Elephant Genotype file, which is 
distributed as a .xlsx file, unfortunately with variations in its name.  
It may be something like:

Elephant_Genotypes_Master_[DBno].xlsx
ElephantMasterFile[DBno].xlsx

If your seizure has not yet been added to this file, you will also need a 
data file containing its genotypes; this will generally be an .xlsx file 
but its name is arbitrary.  (Also ask them what the official name of the 
new seizure is.)

***B)  Obtain the reference data.

If you already have SCAT-style formatted reference data and the 
corresponding Structure results, you can skip this step.

These files would be called:

REFELE_[DBno]_raw.txt                  (raw reference data)
REFELE_[DBno]_known.txt                (formatted reference data)
REFELE_[DBno]_known_structure.txt_f    (Structure results)

If you do not have these files, steps to prepare them are in 
/docs/reference_pipeline.txt.

***C)  Preprocess the data for analysis

1)  Pick a PREFIX (ideally the official name of the seizure) that will 
tag all runs for this case.

2)  Obtain the seizure genotype data.  For a new seizure, open the
spreadsheet and look for a tab labeled "Scat input".  To use data
from older seizures which are already in the database, refer to the 
documentation file master_database_pipeline.txt, which will replace step A3.

3)  Extract the "Scat input" data from the spreadsheet as a .tsv file; this
can be done with LibreOffice by saving the file as .csv but setting the 
delimiter to tab rather than comma, and then changing the name of the
resulting file.  Name this file PREFIX_raw.tsv.

**** Automated pipeline for C4-E4 ****

Steps C4 through E4 can be run by the Python program "phase1.py".
It takes PREFIX as its first argument and either "laptop" or
"cluster" as its second, depending on where you plan to run 
SCAT.  Note that phase1.py itself is not meant to be run on the 
cluster; to use it with the cluster, you would run phase1.py on
your machine, transfer the SCAT directories to the cluster,
and run them there.  Setting the "cluster" flag writes runfiles
that will work with SLURM.

In order to use phase1.py you will need to create a file named
ivory_paths.tsv in the parent directory of all your seizures.  There 
is a prototype of this file in the ivory_pipeline /aux directory.  
This file gives paths and filenames for resources needed by phase1.py 
(and later phases).  It allows the pipeline to be easily run on 
different machines with different file structures, and forces
usage of a consistent set of files (avoiding, for example, running 
half the pipeline with database REFELE_4.3 and half with REFELE_4.4).

There are three kinds of entries in ivory_paths.txt.  Entries ending
in "_dir" should give the full path to the needed directory, ending
in "/".  Entries ending in "_executable" should give the full
path and name of the executable.  Entries ending in "_prefix"
should give, first, the directory in which files of this type are
to be found, and second, the prefix of those files.  For example,
if you are describing the two map files "mapfile_161220_savannah.txt"
and "mapfile_161220_forest.txt", the prefix is "mapfile_161220".
The file is tab-delimited.

After running phase1.py you are ready to start SCAT runs (step E5)
in nforest, nsavannah, or both, depending on what was found in your
seizure.

****

4)  Validate the microsat data by running verifymsat.py with the following
arguments:
--number of msats (currently 16)
--REFELE_[DBno]_raw.csv
--PREFIX_raw.tsv

This program checks for samples with more than 2 previously unknown alleles.
This can indicate that the microsats have gotten out of order; if it fires,
carefully check your input seizure data.  NOTE:  it only checks the seizure 
data, and assumes the reference data are correct.

If you have carefully checked and believe your data are correct (perhaps
your seizure is from a poorly sampled area and has lots of novel alleles), 
you can continue with the next program in the pipeline.

5)  Check for exact matches in input data by running detect_duplicates
with the argument of PREFIX_raw.tsv.  If this fires, there are two or
more samples in the input with identical genotypes at non-missing loci.
These are very likely to be the same animal.  Inclusion of duplicate 
samples  wastes effort in SCAT and is actively misleading in VORONOI and
familial matching:  such duplicates should be culled before contining
(and reported to Yves, the database manager).

5)  Run prep_scat_data.py with an argument of PREFIX. This reads
PREFIX_raw.tsv and writes PREFIX_unknowns.txt.  This program marks the new data 
as region -1, and corrects msats where only one allele is missing to 
mark both as missing.  It also suppresses individuals with less
than 10 successfully typed microsatellites.  (These have often been
suppressed upstream by the author of the spreadsheet, but we double-
check here.)


***D)  Assigning species and identifying hybrids

This step relies on Structure results for the reference data; these
will be called REFELE_[DBno]_known_structure.txt_f.  The Structure
results MUST be for exactly the version of the database that you are
using; even a single elephant change will invalidate them.  If in
doubt, rerun.

1)  Obtain the appropriate allelic dropout file.  This is
currently "dropoutrates_savannahfirst.txt" and is in the auxillary_files 
directory.  (It is a columns reversed version of the original file from 
Mondol et al, which was named "Mondoletal_allelicdropoutrates.txt".)

Note:  In theory, you can create this file for novel data using R scripts, 
but I have never gotten them to run successfully.  The R scripts you would 
use are in the /src directory:

infermarkerspecnullalleleprob.R
infernullalleleprobanderrorprob.R

2)  Copy the following files into the seizure directory:
ivory_pipeline/aux/ebscript_template.R
ivory_pipeline/src/inferencefunctions.R
ivory_pipeline/src/calcfreqs.R
ivory_pipeline/src/likelihoodfunctionsandem.R

(they don't work unless they are all together in one directory)

3)  Run make_eb_input.py with the following arguments.  
-- Structure output file (REFELE_[DBno]_known_structure.txt_f)
-- Reference data (REFELE_[DBno]_known.txt)
-- PREFIX
-- dropoutrates_sacannahfirst.txt

There are three outputs.  

PREFIX_ancestryprops.txt gives the ancestry proportions needed by EBhybrids.  
Be careful with this file; it does not contain sample IDs, but relies on 
being in the same order as the other files written by make_eb_input.py.

PREFIX_plus_ref.txt contains genotype data for the seizure and all reference
individuals (both species).

ebscript.R is the EBhybrids run script modified to point at these files.
Note that if this file exists, it will be overwritten.

4)  Run ebscript.R (command is "Rscript ebscript.R").  This will run
EBhybrids and write four files:  PREFIX_hybt.csv and .txt, and PREFIX_HPs.csv
and .txt.  The hybt files give the probability that the sample is either pure
species or any of 4 hybrid types.  The HPs files give only the probability that
it is a hybrid.  Information is identical between .csv and .txt.

Please note that EBhybrids estimates the probability that a sample is
a hybrid, NOT the proportion of its genome that comes from a given species.


***E) Running SCAT

We do not recommend using the built-in species ranges in SCAT or
the old-style boundary file 316forestboundary.txt.   The programs will
run, but the resulting boundaries are very poor (they include ocean and
exclude valid habitat).  Use up-to-date mapfiles instead.

1)  Make sure you have the following files:

-- Map files of choice: either the "full" map files mapfile_[MAPNO]_[species].txt
or the IUCN map files iucn_[MAPNO]_[species].txt.  These are found in the
/aux subdirectory.

-- Zone files:  zones_[ZONENO]_savannah.txt and zones_[ZONENO]_forest.txt.  
These are not on Github; if you need a copy, ask Mary Kuhner.  The zone 
files (formerly called regionfile) give the mapping between input zone 
name, number, sector, and latitude/longitude.  For input zones with both
forest and savannah individuals, forest individuals use the sector given
in the _forest file and savannah use the sector given in the _savannah file.

If new input zones have been added to the reference data, you will need 
to update these files and assign a new version number.

-- Prototype SCAT and VORONOI run files master_scat_runfile.sh and
master_voronoi_runfile.sh.  These are found in the auxillary_files
directory; they are templates for creating run commands for your
SCAT and VORONOI runs.

OPTIONAL:  If you want a different SCAT run length than our standard one, 
edit the master_scat_runfile.sh file.  The last three numbers (currently 
100 20 100) control the length of the run:  the first is number of sampled 
iterations, the second is steps between samplings, and the third is burnin.
VORONOI assumes exactly 100 sampled iterations and 100 non-sampled 
iterations and would have to be modified to change this, so your best bet is
to change the steps between samplings.  Runtime will increase linearly with 
this (if you double steps between samplings, runtime will approximately 
double).

TO DO:  VORONOI can now handle different numbers of iterations; allow
the pipeline to use this capability.

2)  Run filter_hybrids.py with the following arguments:

-- PREFIX of the run
-- Map prefix, for example mapfile_161220 or iucn_161220 (the part of
the mapfile name before the species name).
-- Zone file prefix, for example zones_40 (the part of the zone file name
before the species name).
-- The path to the SCAT executable including its file name
-- T if the SCAT runs will be done on the cluster, F otherwise

The filter_hybrids.py program will print its hardwired hybrid cutoff to
screen.  Make sure this cutoff is what you want.

This program uses PREFIX_hybt.txt to sort the samples into, maximally, a 
forest and a savannah SCAT input file, called PREFIX_[species].txt;
if there are no unknown individuals from a species, that file will not be 
written.  It silently discards individuals called as hybrids. 

Outputs:

SCAT-format data files:  PREFIX_[species].txt 
SCAT run files:  runfile_[species].sh 

3)  Create a directory for each species-specific SCAT analysis:
the forest directory should be named "nforest" and the savannah directory
"nsavannah".  All subsequent steps will have to be done for both directories, 
if you have both species in your seizure. 

Copy the corresponding SCAT input files (PREFIX_[species].txt) into these 
directories. However, leave the SCAT run files where they are.

Place a SCAT executable in this species-specific SCAT directory.  We 
recommend a clean optimized build from source.  

4)  If you plan to run SCAT on a non-cluster machine, run
setupscatruns.py with the following arguments (twice, if you
have both forest and savannah individuals):

-- name of the species-specific run directory
-- name of the master scat run script (runfile_[species].sh) 
-- random number seed (positive integer)

This seed will be used for the first directory and incremented by 1 for each
subsequent directory.

If instead you plan to run SCAT on the cluster using SLURM,
run cluster_setupscatruns.py (again, twice if both species are 
present) with arguments:

-- PREFIX
-- name of the species-specfic run directory
-- name of the mater scat run script (runfile_[species].sh)
-- random number seed (positive integer)

This will create 9 subdirectories named 1-9 under your named directory, each 
of which contains a SCAT run command file (called runX.sh where X is the 
directory number) and an /outputs directory to hold the results.

5)  To run on your own machine, open a window for each numbered directory and 
execute the runX.sh file (using "source" as they are not executable files).  
This should run SCAT nine times.  It may take hours or days to run.

To run on the cluster, consult cluster documentation.

TO DO:  document running on the cluster.


***F)  Running VORONOI

Do not do this step unless there were at least 2 samples in the species
under consideration:  running VORONOI on one sample is not useful.
You will need to execute steps F and G once for each species present
in the seizure.

TO DO:  the script will run this even with one sample.  It seems harmless
but there is a risk that the resulting VORONOI outputs will be used for
something.  This should be blocked.

**** Automated pipeline for F-G ****

The script phase2.py will run steps F and G.  It should be run
in the parent directory of all seizures, and expects to find
ivory_paths.tsv in that directory (see instructions for the phase1.py
step for instructions on ivory_paths.tsv).  It takes PREFIX as its
argument.

****

1)  Obtain or prepare a masterfile consisting of 9 lines as follows (there
is an example, called "masterfile", in the auxillary_files directory):

1/outputs
2/outputs 
...

2)  In the species-specific directory, run setupvoronoi.py with
arguments PREFIX and species name (savannah or forest).  This sets
up for a new-style VORONOI run in which SCAT results are read directly
from the directories that contain them, rather than being recoded and
copied into the working directory.  (We STRONGLY discourage use of
the old format, which leads all too easily to getting confused about
which sample is which as they are not named.)

2)  In the species-specific directory, run scat2voronoi.py
with the argument being the masterfile from step 1.  This will
write two files:

a)  A file named PREFIX_names.txt which gives the names of all
unknown-location (seizure) samples for this species.

b)  A run file named voronoi_runfile_[species].sh.

3)  Pull a VORONOI executable:  we recommend a clean optimized build from
source.

4)  Run VORONOI:

source voronoi_runfile_[species].sh

It should take no more than 30 minutes for most data.


***G)  Post-processing

1)  Run plot_scat_vor.py (found in ivory_pipeline/src) in the directory 
where you ran VORONOI.  It takes 1 or 2 arguments.  If there is only 1 
it is PREFIX, and all samples will be run.  If there are 2, they are 
PREFIX and SID, and only the sample with that SID will be run. 

This writes several types of files into directory PREFIX_reports:

-- heatmaps of the SCAT and VORONOI results for individual samples
-- scat_summary_medians.jpg (SCAT results as median latitude/longitude)
-- scat_summary_squares.jpg (SCAT results as grid square of highest count)
-- voronoi_sumary.jpg (VORONOI results as grid square of highest posterior)
-- PREFIX_point_estimates.tsv (best estimates for each sample)

To do:  make this program work if there are no VORONOI results, for
example because there was only 1 sample.  plot_scat.py used to do 
this but would need extensive rewriting; better to make plot_scat_vor.py do it.

TO DO:  write a note on how to install Cartopy:  it's tricky.


***H)  Run familial matching

This document explains how to do familial matching of a new seizure when
all previous seizures have already been run and archived.  If
you need to do familial matching on the entire data set at once, see 
documentation file familial_pipeline.txt, and be aware that it will 
take 3+ days.  Rerunning the whole thing is needed if large changes 
have been made to the reference data, the sector definitions have 
changed, or the set of previous seizures to be used has changed.

WARNING: Step H2 uses files from previous seizures.  Do not run step H2 on more 
than one seizure at a time!  (Other steps are okay.)

** OPTIONAL **

The following steps (H1-H4) can be done by the script mary_fammatch.sh on Mary's
laptop.  It is meant to be run in the parent director of all the seizure 
directories, and takes PREFIX as its argument.  Note that the script spawns
jobs for the familial matching runs, and when it says it is finished these
may still be running.  You may safely run the same program on a *different
seizure* once it terminates on the current seizure, but you must NOT do 
postprocessing on the current seizure until all of the jobs terminate.

Note that running mary_fammatch.sh will destroy the current /fammatch subdirectory
for this seizure.

1)  Make a directory "fammatch" in your main PREFIX directory (not
nforest/nsavannah).  Make two subdirectories within it for SCAT sector
assignment runs:  "outdir_forest" and "outdir_savannah".

For each species present in your seizure you will need to do a SCAT
run here:

./SCAT -Z -H2 ../PREFIX_[species]_plus_ref.txt ../zones_species.txt outdir_[species] 16

This will compute sector (previously called "subregion") probabilities for all 
reference and seizure samples.  The seizure samples will be classified into 
sectors based on these probabilities:  the reference samples will instead be 
classified based on their sector in the regionfile, but must be included here
to establish the sector allele frequencies.  It runs in minutes.

2)  Obtain make_fammatch_incremental.py from the ivory_pipeline /src
directory.  Check to make sure it has the correct value for the
nsub variable (number of sectors, currently 6).  Run this
program for each species for which you have samples, using the following
arguments:

PREFIX
outdir_[species]
path to familial matching input archive (currently ~/data/fammatch_input
   on Mary's laptop)
regionfile_[species]

This will create subdirectories for each sector that has samples
in the seizure.  It will also add the samples from the new seizure to
the archive for further use.  (Therefore, if you are running on a 
copy of the archive, you will need to update the original.  This can
be done by finding all files in the archive that begin with PREFIX
and adding them to the canonical archive, in the proper subdirectories.)  

One unusual case can occur:  If there is exactly one
sample for a particular sector in this seizure, and no samples for
the sector in the archives, familial matching cannot be run.  This
will be signaled by the presence of a file ONLY_ONE_SAMPLE in the
subdirectory for that sector (and absence of some of the run files).
The sample will be archived as usual and will participate in subsequent
familial matching.

This program also writes run files for familial matching into each
sector directory, called "runrscript.sh".

Nothing below this point separates the species (as samples have been
divided into sectors, and the species information is implicit in the
sector).

3)  Place copies of calculate_LRs.R and LR_functions.R in each
sector directory.

4)  In each sector directory, source runrscript.sh (no arguments).
This will contain a run command of the form:

Rscript calculate_LRs.R [species] refN_fammatch.csv oldN.txt newN.txt

Runtime depends on the size of the seizure but usually not more than
a few hours and often much less.  These runs do not interact so you
can run all sectors at the same time (as the script does).


***I)  Postprocess familial matching

1) OPTIONAL.  To simply view the results of familial matching on your new 
seizure, you can run Charles' Python scripts in each sector directory for
which you had samples in your seizure.  These are found in the /src directory.
If no matches are found, there is no point continuing with network analysis.
Be careful, however, not to treat matches found this way as conclusive; a
large proportion will be false positives.

python 1_add_seizures.py --input_file obsLRs.[species].txt --seizure_file seizure_metadata.tsv

This writes obsLRs.[species].seizures.txt which is annotated with seizure
names.  As it does not use seizure_modifications, it will not necessarily get the
names right.

python 2_filter_results.py --input_file obsLRs.[species].seizures.txt --cutoff 2.0

Note that if you mistakenly use obsLRs.[species].txt as input, this program will
appear to work fine, but downstream code will NOT.   It writes
obsLRs.[species].2.0.filtered.txt, which can be consulted to find all the putative
significant matches involving your new seizure.

python 3_seizure_analysis.py --filtered_file obsLRs.[species].seizures.2.0.filtered.txt

This will write obsLRs.[species].seizures.2.0.filtered.seizanalysis.txt, which
summarizes comparisons between seizures.  For interpreting the results you
will use both obsLRs.[species].2.0.filtered.txt and this file.

(There is a fourth script distributed with the familial matching code,
4_remove_seizures.py, but this pipeline does not use it.  It is also
present in src if you find a use for it.)

2)  Obtain the file seizure_metadata_[DBno].tsv (this is not kept
on the Github site; ask Mary Kuhner for a copy).  Make a COPY of this
file and run update_metadata.py on the copy (this program changes its
input file in place!)  The three arguments are the metadata (copy) file,
the PREFIX_unknowns.txt file, and the official name of your new seizure.
Check that the results are sensible (your new seizure should be at
the bottom of the file) before overwriting the original.  If you have 
changed [DBno] be sure to change the name of this file.  Return the
modified file to Mary for archiving.

This step is not needed and should not be done if this seizure was previously
run and you are rerunning it for some reason; it is only for a genuinely
new seizure (not already in seizure_metadata).

3)  Obtain or create a seizure_modifications file.  This file contains
information on any seizures to be excluded from analysis or merged
together.  A copy of the current version, seizure_modifications_nature,
is in the auxillary_files directory.  It reflects the seizure changes
made for the paper submitted to Nature in May 2021.

This file has two sections.  The first is introduced with REJECT
on a line by itself and lists seizures to be rejected, one per line.
The second is introduced with MERGE on a line by itself and
introduces tab-separated lists of seizure names.  The first name will
be used as the name of the merged seizure, and the remainder will be
merged.  Note that you can use this to rename a seizure by giving
two entries, the first being the new name and the second the old
name.

It is likely that if you change this file at all, except as it pertains
to the new seizure you are running, you will have to rerun the
entire familial matching analysis as described in familial_pipeline.txt.
You *may* be able to reject more seizures than were rejected when your
previous familial matching runs were done, though this is not tested;
similarly, you may be able to rename old seizures.  You certainly 
cannot reject fewer old seizures or merge old seizures with each other 
or with the new ones.

4)  Obtain results of previous familial matching.  They will take the form of 
files named obsLRs_[species]_N.txt, where N is the sector number.  These are
archived by Mary.  Place them in the directory which contains your sector
directories sub0, sub1 etc.

Obtain a file of false-positive results from simulated data, called fprates.tsv;
a copy is in auxillary_files.  If the sectoring scheme is changed, these will
need to be regenerated.

TO DO:  document simulation pipeline used to make these false-positive rates.

Obtain a copy of the canonical list of direct (exact) matches, called dms.tsv.
This is not kept on Github; ask Mary for a copy.  This file is used because we 
recognize as direct matches some imperfect matches which have been detected by
allelic-dropout-aware code such as CERVUS, whereas allowing for imperfect
matches in the familial matching code would make it too slow.

5)  Run consolidate_fammatch.py to merge old and new results.

6)  Run create_network_input.py to calculate weighted matches between seizures.
Its arguments are:

seizure_metadata.txt
dmfile.tsv
seizure_modifications
log(LR) cutoff (standard is 2.0)
minimum typed loci in common (standard is 13)

The program also silently reads fprates.tsv, which must be in its run directory;
and it contains hardcoded path names for the obsLRs files (see line 63 and following)
which will have to be changed if you use a different directory setup.

It writes seizure_nodes.csv and seizure_edges.csv, which contain the weighted
matches between each pair of seizures.  It also writes two matrices of counts
between pairs of seizures, broken out by forest and savannah, into a single
file named "ryanfile" used by Ryan's graphical software.

If you simply want to know about all weighted matches involving the new seizure,
grep for its name in seizure_edges.csv.

If this program fails with a KeyError message, there is a sample ID in your
familial matching results which is not in your seizure_metadata file.  Did
you forget to update this file?  Is it a sample which was supposed to be
deleted, but was not?

7)  To infer a Louvain network and make a graphic of it, run the Python program
infer_louvain_network.py.  Arguments are:

minlink -- connections with weight below this are dropped.  Was 1.0 in paper.
seizure_modifications

There are fancier versions of this program which pull in forensic data, Ryan's
data, etc. but this is the basic version.
