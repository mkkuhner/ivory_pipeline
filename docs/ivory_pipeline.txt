Pipeline for analyzing new elephant ivory seizure data

This pipeline is specific for African elephants with the 16
microsatellite data set.  It will need modification to accomodate
other species or types of data.  (In particular, the step using
EBhybrids requires exactly two species:  a substitute will be
needed for pangolins if they prove to have hybrids.)

Code and reference data for this pipeline are in the private GitHub
repository https://github.com/mkkuhner/ivory_pipeline.  Within that
repository, programs and scripts are in the /src directory,
additional files needed to run the scripts are in the /aux
directory, and documentation files are in the /docs directory.

WARNING:  Never check anything into the GitHub repository which is
law enforcement sensitive or otherwise high security.  

The pipeline also relies on the following code found in other
respositories:

SCAT -- https://github.com/stephens999/scat
VORONOI -- https://github.com/stephens999/voronoi
EBhybrids -- https://github.com/stephenslab/EBhybrids
Familial Matching -- https://github.com/cwolock/elephant_fam_match
Structure -- https://web.stanford.edu/group/pritchardlab/structure_software/release_versions/v2.3.4/html/structure.html

Currently the ivorydata/src directory does not include Structure, SCAT or
VORONOI, but does include copies of EBhybrids and Familial Matching.
If these programs are updated, replace these copies!

***General Rules

1)  All Python scripts are Python3.

2)  The term [species] refers to either "savannah" or "forest".

3)  The term [date] refers to a date in format YYYY_MM_DD (e.g.  2021_01_30).

4)  The term [DBno] refers to the numeric ID of a release of the Master Elephant 
Genotypes file.

***Automated Scripts

This document covers running the pipeline by hand.  To run it using automated 
scripts, see document ivory_scripts.txt.


***A)  Setup

1)  If you do not have up-to-date installations of the following programs:

SCAT
VORONOI
EBhybrids
Familial Matching
Structure

obtain and install them now.  Detailed instructions are in document 
installing_software.txt in the ivory_pipeline directory.  This document
assumes that the SCAT and VORONOI executables are named SCAT and VORONOI;
adjust appropriately if they are not. 

2)  Obtain the needed data files.  Files with elephant genotypes, reference or 
seizure, are not kept in the online repositories for security reasons.  Yves 
Horeau can provide the most recent copy of the master elephant genotypes file, 
which can be used as a source for reference data and previously analyzed 
seizure data.  Data for a brand new seizure is generally not yet in this file; 
it should be requested from the laboratory staff who genotyped it.

You will minimally need the Master Elephant Genotype file, which is 
distributed as a .xlsx file, unfortunately with variations in its name.  
It may be something like:

Elephant_Genotypes_Master_[DBno].xlsx
ElephantMasterFile[DBno].xlsx

If your seizure has not yet been added to this file, you will also need a 
data file containing its genotypes; this will generally be an .xlsx file 
but its name is arbitrary.  (Also ask Yves what the official name of the 
new seizure is.)

3)  Create a pathfile which gives paths to information needed by the pipeline.
This should be a .tsv (we often call it ivory_paths.tsv and will use this name
throughout the documentation).  Multiple programs in the pipeline rely on the 
pathfile to locate needed resources.

A sample ivory_paths.tsv file can be found in

ivory_pipeline/aux/ivory_paths.tsv

but will need to be modified for your own directory structure.

Lines within the pathfile can be in any order; entries within the same line must 
be in order and must be separated with tabs.  Directory names must end in "/".
The first entry of each line names a particular required directory or file.  The second 
entry gives a path to find it; for lines with a third entry, this gives the prefix of 
the file (useful when there are several related files such as _forest and _savannah variants).

Legal lines in the pathfile:

ivory_pipeline_dir      path to the ivory_pipeline root directory
scat_executable         path and filename of the SCAT executable
voronoi_executable      path and filename of the VORONOI executable
reference_prefix        path to the reference data files        prefix of these files
zones_prefix            path to zones files                     prefix of these files
metadata_prefix         path to seizure metadata                prefix of this file
seizure_modifications_prefix    path to modifications file      prefix of this file
map_prefix              path to map files                       prefix of these files
seizure_data_dir        directory where raw seizure data is kept
structure_executable    path and filename of the Structure executable
fammatch_archive_dir    path to familial matching input file archive

TO DO:  get rid of metadata_prefix as this file can be found in fammatch_archive_dir.

***B)  Obtain the reference data.

If you already have SCAT-style formatted reference data and the 
corresponding Structure results, you can skip this step.

These files would be called:

REFELE_[DBno]_raw.txt                  (raw reference data)
REFELE_[DBno]_known.txt                (formatted reference data)
REFELE_[DBno]_known_structure.txt_f    (Structure results)
REFELE_[DBno]_filtered_[species].txt   (hybrid-filtered species reference)

If you do not have these files, steps to prepare them are in 
/docs/reference_pipeline.txt.

***C)  Preprocess the data for analysis

1)  Pick a PREFIX (ideally the official name of the seizure) that will 
tag all runs for this case.  (If you find out later that you had the
wrong name, program renameseizuredir.py can be used to rename a seizure.)

2)  Obtain the seizure genotype data.  For a new seizure, open the
spreadsheet and look for a tab labeled "Scat input".  To use data
from older seizures which are already in the database, refer to the 
documentation file master_database_pipeline.txt, which will replace step A3.

3)  Extract the "Scat input" data from the spreadsheet as a .tsv file; this
can be done with LibreOffice by saving the file as .csv but setting the 
delimiter to tab rather than comma, and then changing the name of the
resulting file.  Name this file PREFIX_raw.tsv.

4)  Run the logging program log_seizure.py to record the parameters under
which the run was done.  The run command is:
python3 log_seizure.py PREFIX ivory_paths.tsv

4)  Validate the microsat data by running verifymsat.py with the following
arguments:
--number of msats (currently 16)
--REFELE_[DBno]_raw.csv
--PREFIX_raw.tsv

This program identifies samples with more than 2 previously unknown alleles.
If it fires, carefully check your input seizure data for ordering or
calibration problems.  NOTE:  it only checks the seizure data, and assumes 
the reference data are correct.

If you have examined your data and verified that the unknown alleles are
correct (perhaps your seizure is from a poorly sampled area and has lots 
of novel alleles), you can continue with the next program in the pipeline.
Subsequent programs are prepared to handle unknown alleles, but we check
here as they may indicate a systematic data quality issue.

5)  Check for exact matches in input data by running detect_duplicates
with the argument of PREFIX_raw.tsv.  If this fires, there are two or
more samples in the input with identical genotypes at non-missing loci.
These are very likely to be the same animal.  Inclusion of duplicate 
samples wastes effort in SCAT and is actively misleading in VORONOI and
familial matching:  such duplicates should be culled before continuing
(and reported to Yves, the database manager).

5)  Run prep_scat_data.py with an argument of PREFIX. This reads
PREFIX_raw.tsv and writes PREFIX_unknowns.txt.  This program marks the new data 
as region -1, and corrects msats where only one allele is missing to 
mark both as missing.  It also suppresses individuals with less
than 10 successfully typed microsatellites.  (These have often been
suppressed upstream by the author of the spreadsheet, but we double-
check here.)


***D)  Assigning species and identifying hybrids

This step relies on Structure results for the reference data; these
will be called REFELE_[DBno]_known_structure.txt_f.  The Structure
results MUST be for exactly the version of the database that you are
using; even a single elephant change will invalidate them.  If in
doubt, rerun.

1)  Obtain the appropriate allelic dropout file.  This is currently 
"dropoutrates_savannahfirst.txt" and is in the ivory_pipeline/aux 
directory.  (It is a columns reversed version of the original file from 
Mondol et al, which was named "Mondoletal_allelicdropoutrates.txt".)

Note:  In theory, you can create this file for novel data using R scripts, 
but I have never gotten them to run successfully.  The R scripts you would 
use are in the /src directory:

infermarkerspecnullalleleprob.R
infernullalleleprobanderrorprob.R

2)  Copy the following files into the seizure directory:
ivory_pipeline/aux/ebscript_template.R
ivory_pipeline/src/inferencefunctions.R
ivory_pipeline/src/calcfreqs.R
ivory_pipeline/src/likelihoodfunctionsandem.R

(they don't work unless they are all together in one directory)

3)  Run make_eb_input.py with the following arguments.  
-- Structure output file (REFELE_[DBno]_known_structure.txt_f)
-- Reference data (REFELE_[DBno]_known.txt)
-- PREFIX
-- dropoutrates_sacannahfirst.txt

There are three outputs.  

PREFIX_ancestryprops.txt gives the ancestry proportions needed by EBhybrids.  
Be careful with this file; it does not contain sample IDs, but relies on 
being in the same order as the other files written by make_eb_input.py.

PREFIX_plus_ref.txt contains genotype data for the seizure and all reference
individuals (both species).

ebscript.R is the EBhybrids run script modified to point at these files.
Note that if this file exists, it will be overwritten.

4)  Run ebscript.R (command is "Rscript ebscript.R").  This will run
EBhybrids and write four files:  PREFIX_hybt.csv and .txt, and PREFIX_HPs.csv
and .txt.  The hybt files give the probability that the sample is either pure
species or any of 4 hybrid types.  The HPs files give only the probability that
it is a hybrid.  Information is identical between .csv and .txt.

Please note that EBhybrids estimates the probability that a sample is
a hybrid, NOT the proportion of its genome that comes from a given species.

TO DO:  integrate the hybrid report program here

***E) Running SCAT

We do not recommend using the built-in species ranges in SCAT or
the old-style boundary file 316forestboundary.txt.   The programs will
run, but the resulting boundaries are very poor (they include ocean and
exclude valid habitat).  Use up-to-date mapfiles instead.

1)  Make sure you have the following files:

-- Map files of choice: either the "full" map files mapfile_[MAPNO]_[species].txt
or the IUCN map files iucn_[MAPNO]_[species].txt.  These are found in the
ivory_pipeline/aux subdirectory.

-- Zone files:  zones_[ZONENO]_savannah.txt and zones_[ZONENO]_forest.txt.  
These are not on Github; if you need a copy, ask Mary Kuhner.  The zone 
files (formerly called regionfile) give the mapping between input zone 
name, number, sector, and latitude/longitude.  For input zones with both
forest and savannah individuals, forest individuals use the sector given
in the _forest file and savannah use the sector given in the _savannah file.

If new input zones have been added to the reference data, you will need 
to update these files and assign a new version number.

-- Prototype SCAT and VORONOI run files master_scat_runfile.sh and
master_voronoi_runfile.sh.  These are found in the ivory_pipeline/aux
directory; they are templates for creating run commands for your
SCAT and VORONOI runs.

OPTIONAL:  If you want a different SCAT run length than our standard one, 
edit the master_scat_runfile.sh file.  The last three numbers (currently 
100 20 100) control the length of the run:  the first is number of sampled 
iterations, the second is steps between samplings, and the third is burnin.
VORONOI assumes exactly 100 sampled iterations and 100 non-sampled 
iterations and would have to be modified to change this, so your best bet is
to change the steps between samplings.  Runtime will increase linearly with 
this (if you double steps between samplings, runtime will approximately 
double).

TO DO:  VORONOI can now handle different numbers of iterations; allow
the pipeline to use this capability.

2)  Filter out hybrids and construct species-specific files.

The hybrid identification program EBhybrids is stochastic (EM algorithm)
and elephants who are very close to the hybrid cutoff will randomly be
called hybrids or not if the program is rerun.  This creates a problem when
the hybrid-culled reference data sets are being built, as different seizures
will end up with slightly different reference sets.  This breaks familial
matching downstream.  We therefore use "canned" reference data, made
by running filter_hybrids.py with the "T" flag for use_canned_reference.
If you do not have canned reference data you can make them by running
filter_hybrids.py with the "F" flag instead; they should then be named
REFELE_[DBno]_filtered_[species].txt and placed in the data directory.
This should be done only ONCE per species per database version to avoid
incoherence.

Run filter_hybrids.py with the following arguments:

-- PREFIX of the run
-- T if the SCAT runs will be done on the cluster, F otherwise
-- T if the canned reference should be used, F otherwise

The filter_hybrids.py program will print its hardwired hybrid cutoff to
screen.  Make sure this cutoff is what you want.

This program uses PREFIX_hybt.txt to sort the samples into, maximally, a 
forest and a savannah SCAT input file, called PREFIX_[species].txt;
if there are no unknown individuals from a species, that file will not be 
written.  It silently discards individuals called as hybrids. 

TO DO:  Is it still "silently"?

Outputs:

SCAT-format data files:  PREFIX_[species].txt 
SCAT run files:  runfile_[species].sh 

3)  Create a directory for each species-specific SCAT analysis:
the forest directory should be named "nforest" and the savannah directory
"nsavannah".  All subsequent steps will have to be done for both directories, 
if you have both species in your seizure. 

Copy the corresponding SCAT input files (PREFIX_[species].txt) into these 
directories. However, leave the SCAT run files where they are.

Place a SCAT executable in this species-specific SCAT directory.  We 
recommend a clean optimized build from source.  

4)  If you plan to run SCAT on a non-cluster machine, run
setupscatruns.py in the main PREFIX directory with the following arguments (twice, 
if you have both forest and savannah individuals):

-- name of the species-specific run directory (nforest or nsavannah)
-- name of the master scat run script (runfile_[species].sh) 
-- random number seed (positive integer)

This seed will be used for the first directory and incremented by 1 for each
subsequent directory.

If instead you plan to run SCAT on the cluster using SLURM,
run cluster_setupscatruns.py (again, twice if both species are 
present) with arguments:

-- PREFIX
-- name of the species-specfic run directory
-- name of the mater scat run script (runfile_[species].sh)
-- random number seed (positive integer)

This will create 9 subdirectories named 1-9 under your named directory, each 
of which contains a SCAT run command file (called runX.sh where X is the 
directory number) and an /outputs directory to hold the results.

5)  To run on your own machine, open a window for each numbered directory and 
execute the runX.sh file (using "source" as they are not executable files).  
This will run SCAT nine times.  It may take hours or days to run.

To run on the cluster, consult cluster documentation.

TO DO:  document running on the cluster.


***F)  Running VORONOI

Do not do this step unless there were at least 2 samples in the species
under consideration:  running VORONOI on one sample is not useful.
You will need to execute steps F and G once for each species present
in the seizure.

TO DO:  the script will run this even with one sample.  It seems harmless
but there is a risk that the resulting VORONOI outputs will be used for
something.  This should be blocked.

TO DO:  VORONOI is actually bad with < 10 samples; adjust accordingly.

1)  Obtain or prepare a masterfile consisting of 9 lines as follows (there
is an example, called "masterfile", in the ivory_pipeline/aux directory):

1/outputs
2/outputs 
...

2)  In the species-specific directory, run setupvoronoi.py with arguments 
PREFIX, species name (savannah or forest), and the ivory_paths.tsv file.  
This sets up for a new-style VORONOI run in which SCAT results are read directly
from the directories that contain them, rather than being recoded and
copied into the working directory.  (We STRONGLY discourage use of
the old format, which leads all too easily to getting confused about
which sample is which as they are not named.)

3)  From nforest or nsavannah, run VORONOI using the run command file written
by setupvoronoi.py:

source voronoi_runfile_[species].sh

It should take no more than 30 minutes for most data.


***G)  Post-processing

1)  Run plot_scat_vor.py (found in ivory_pipeline/src) in the directory 
where you ran VORONOI.  It takes 1 or 2 arguments.  If there is only 1 
it is PREFIX, and all samples will be run.  If there are 2, they are 
PREFIX and SID, and only the sample with that SID will be run. 

This writes several types of files into directory PREFIX_reports:

-- heatmaps of the SCAT and VORONOI results for individual samples
-- scat_summary_medians.jpg (SCAT results as median latitude/longitude)
-- scat_summary_squares.jpg (SCAT results as grid square of highest count)
-- voronoi_sumary.jpg (VORONOI results as grid square of highest posterior)
-- PREFIX_point_estimates.tsv (best estimates for each sample)

TO DO:  make this program work if there are no VORONOI results, for
example because there was only 1 sample.  plot_scat.py used to do 
this but would need extensive rewriting; better to make plot_scat_vor.py do it.

TO DO:  write a note on how to install Cartopy:  it's tricky.


***H)  Run familial matching

This document covrs familial matching of a new seizure when
all previous seizures have already been run and archived.  If
you need to do familial matching on the entire data set at once, see 
documentation file familial_pipeline.txt, and be aware that it will 
take 3+ days.  Rerunning the whole thing is needed if large changes 
have been made to the reference data, the sector definitions have 
changed, or the set of previous seizures to be used has changed.

Familial matching results for all seizures are archived in the fammatch
archive directory (currently on the external HD) as a file called 
elephant_msat_database.tsv.

I do not recommend doing step H by hand.  The scripts that do it automatically
make elaborate backups at each step to avoid corrupting the archive; if you
do it by hand it is easy to make a mistake and have to rerun the entire
familial matching process on all seizures.

WARNING:  These instructions assume this seizure has not been run before.  
If it has, but you have modified it and want to rerun, you need to remove
the previous results from the archive.  This is done with 
remove_seizure_from_fammatch.py, which takes PREFIX as its argument.

1)  Update the seizure metadata (a file listing the seizure of each sample ID).
This file is found in the same directory as the database file; if the file
does not exist, this program will create it.  The command is:

python3 update_metadata.py seizure_metafile PREFIX

2)  Use SCAT to assign seizure samples to sectors.  Make a directory "fammatch" 
in your main PREFIX directory (not nforest/nsavannah).  Make two subdirectories 
within it for SCAT sector assignment runs:  "outdir_forest" and "outdir_savannah".  
For each species present in your seizure do a SCAT run (in the main PREFIX 
directory) with the following run command:

./SCAT -Z -H2 ../PREFIX_[species]_plus_ref.txt ../zones_[species].txt outdir_[species] 16

This will compute sector (previously called "subregion") probabilities for all 
reference and seizure samples.  The seizure samples will be classified into 
sectors based on these probabilities:  the reference samples will instead be 
classified based on their sector in the regionfile, but must be included here
to establish the sector allele frequencies.  It runs in minutes.

3)  Set up familial matching.  Return to the root directory of all seizures and run
prep_fammatch.py with the following arguments:  PREFIX,
zone file prefix, zone file path.  This prepares the familial matching
directories for running.

TO DO:  prep_fammatch.py should use ivory_paths rather than taking these
arguments.

4)  Run familial matching.  Still in the root directory of all seizures,
run run_fammatch.py with the following arguments:  PREFIX, ivory_paths.tsv.
This can take hours if many previous seizures are available.  It is written
in an inefficient but safe way to run only one sector at a time; it could
be speeded up (for seizures with samples from many sectors) by parallelizing
this.

One unusual case can occur:  If there is exactly one
sample for a particular sector in this seizure, and no samples for
the sector in the archives, familial matching cannot be run.  This
will be signaled by the presence of a file ONLY_ONE_SAMPLE in the
subdirectory for that sector (and absence of some of the run files).
The sample will be archived as usual and will participate in subsequent
familial matching.

5)  Update the log of familial-matched seizures.  This is in the same
directory as the archive and is called "seizurelist.tsv".  Simply
add PREFIX, on its own line, to the end.


***I)  Postprocess familial matching

TO DO:  write a single-seizure interim report.

1) OPTIONAL.  To simply view the results of familial matching on your new 
seizure, you can run Charles' Python scripts in each sector directory for
which you had samples in your seizure.  These are found in the /src directory.
If no matches are found, there is no point continuing with network analysis.
Be careful, however, not to treat matches found this way as conclusive; a
large proportion will be false positives.

python 1_add_seizures.py --input_file obsLRs.[species].txt --seizure_file seizure_metadata.tsv

This writes obsLRs.[species].seizures.txt which is annotated with seizure
names.  As it does not use seizure_modifications, it will not necessarily get 
the names right.

python 2_filter_results.py --input_file obsLRs.[species].seizures.txt --cutoff 2.0

Note that if you mistakenly use obsLRs.[species].txt as input, this program will
appear to work fine, but downstream code will NOT.   It writes
obsLRs.[species].2.0.filtered.txt, which can be consulted to find all the putative
significant matches involving your new seizure.

python 3_seizure_analysis.py --filtered_file obsLRs.[species].seizures.2.0.filtered.txt

This will write obsLRs.[species].seizures.2.0.filtered.seizanalysis.txt, which
summarizes comparisons between seizures.  For interpreting the results you
will use both obsLRs.[species].2.0.filtered.txt and this file.

(There is a fourth script distributed with the familial matching code,
4_remove_seizures.py, but this pipeline does not use it.  It is also
present in src if you find a use for it.)

The remaining steps are for a full, formal familial matching analysis.

2)  Obtain or create a seizure_modifications file.  This file contains
information on any seizures to be excluded from analysis or merged
together.  A copy of the current version, seizure_modifications_nature,
is in the /aux directory.  It reflects the seizure changes
made for the paper submitted to Nature Human Behavior in May 2021.

This file has two sections.  The first is introduced with REJECT
on a line by itself and lists seizures to be rejected, one per line.
The second is introduced with MERGE on a line by itself and
contains tab-separated lists of seizure names.  The first name will
be used as the name of the merged seizure, and the remainder will be
renamed to that name.  Note that you can use this to rename a seizure 
by giving two entries, the first being the new name and the second the old
name.

If you do not want to reject or merge any seizures, the file can consist
of just the two lines REJECT and MERGE.

TO DO:  Does the pipeline correctly handle matches between seizures which
are now being merged?

3)  Obtain a file of false-positive rates from simulated data, called
fprates.tsv; the current version is in ivory_pipeline/aux.  If the sectoring
scheme is changed these will need to be regenerated.

TO DO:  document simulation pipeline used to make these false-positive rates.

Obtain a copy of the canonical list of direct (exact) matches, called dms.tsv.
This is not kept on Github; ask Mary for a copy.  This file is used because we 
recognize as direct matches some imperfect matches which have been detected by
allelic-dropout-aware code such as CERVUS.  We do not currently allow for 
imperfect matches in the familial matching code for speed reasons.

4)  The next steps cannot currently be run individually by hand as essential
code exists only in the main scripts.  Also, the main script manages
critical backups to prevent database corruption.  Therefore, the next
step is to run phase4.py with the following inputs:

ivory_paths.tsv
dms.tsv
LR_cutoff (we normally use 2.0)
minloci (minimum loci in a valid comparison; we normally use 13)

5)  Prepare a seizure numbering file that gives numbers for all the
seizures you will use, and also gives the country code of the port of origin 
for each seizure (if port is not known, code as UNK).  The easiest way
to do this is to add your seizure at the end of the current 
seizure_numbering_[date].tsv file found in ivory_pipeline/aux, with a
placeholder for the seizure number and the correct port.  Then run
number_seizures.py with arguments of the current seizure numbering
file and the desired new seizure numbering file.  This will renumber the
seizures as needed to make them chronological. 

NB:  This program assumes that all seizure names are of the form
CCC_MM_YY_n.nt, possibly followed by an additional letter.  If your
seizure is not named in that way, you will have to update seizure_numbering
by hand. 

NB:  The chronological numbering assumes that seizure years are either
1950-1999 or 2000-2049, because the seizure names use 2 digit years.
If you have a seizure prior to 1950, you will have to renumber by hand.
There are no such seizures in the database as of 4/21/22 but one could
conceivably be assembled from museum specimens.

5)  To cluster seizures and make a network diagram, run draw_network.py
with the following arguments:

minlink (minimum weight of a link to be drawn; our default is 1.0)
ivory_paths.tsv
seizure_numbering.tsv
"None"

The final argument is a layout file (typically called layout.pkl) which
allows you to re-use a nice layout from a previous attempt.  However, if
any seizures have been added or removed this is not possible and you
must pass None.

This program is interactive.  It will display the Louvain clustering
network, colored by clustering partition, which is generally hideous in its 
default form as nodes will overlay each other; use the mouse to rearrange 
the nodes.  Your rearrangements will be incorporated into the graphics
files written by the program.

It will then write five files.  Four are images of the network:

network_graph_partition.svg -- colored by partition, edges shown
network_graph_partition_nolines.svg -- colored by partition, edges not shown
network_graph_port.svg -- colored by port, edges shown
network_graph_port_nolines.svg -- colored by port, edges not shown

(Ryan uses the no-edges version to prepare graphics for publication.)

The fifth file is a layout file, "new_layoutfile.pkl", which will reproduce
the spatial layout of the nodes.  It does not contain information about
colors or line thicknesses.  If you decide you don't like the
current layout, rerunning using this as the layout file and then
rearranging further by mouse is helpful.  Note that if you do not use a
layout file, the visual output of this program is highly stochastic and
will never be the same twice.
